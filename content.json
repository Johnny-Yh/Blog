{"pages":[{"title":"About","text":"SCNU理工男，喜爱各种电子产品","link":"/about/index.html"},{"title":"categories","text":"Linux考研杂谈","link":"/categories/index.html"},{"title":"tags","text":"Linux爬虫","link":"/tags/index.html"}],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/01/12/hello-world/"},{"title":"在Linux下修改Swap分区大小","text":"今天无意间看到主机的swap的大小只有131M，而且占用也快满了，想要修改swap分区大小，改为两倍系统物理内存的大小（1024M），以满足日常使用的需求。 swap分区：通常被称为交换分区，这是一块特殊的硬盘空间，即当实际内存不够用的时候，操作系统会从内存中取出一部分暂时不用的数据，放在交换分区中，从而为当前运行的程序腾出足够的内存空间。 使用free命令查看系统内存的使用情况： [root@PC ~]# free -m total used free shared buff/cache available Mem: 493 184 112 6 195 289 Swap: 131 0 131 可以看到我们这台主机内存有493M，而swap分区只有131M（我之前重新挂载过一次分区，所以当前使用为0M） 使用swapon命令查看swap分区： [root@PC ~]# swapon -s Filename Type Size Used Priority /swap file 135164 0 -2 可以看到swap分区的位置为/swap 使用swapoff命令关闭swap分区： [root@PC ~]# swapoff -a 再次查看系统内存： [root@PC ~]# free -m total used free shared buff/cache available Mem: 493 212 41 6 239 261 Swap: 0 0 0 可以看到swap分区的空间已经变成了0删除当前的swap分区文件： [root@PC ~]# rm /swap rm: remove regular file '/swap'? y 重新创建swap分区：dd命令的部分参数如下：参数bs为一个块的大小，参数count为总的块数 [root@PC ~]# dd if=/dev/zero of=/swap count=1024 bs=1M 1024+0 records in 1024+0 records out 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 3.59162 s, 299 MB/s 查看swap分区： [root@PC ~]# ls / | grep swap swap 可以看到swap已经成功重新创建 为swap文件赋予权限（文件所有者的读写权限）： [root@PC ~]# chmod 600 /swap 查看swap文件权限： [root@PC ~]# ls -lh /swap -rw------- 1 root root 1.0G Mar 17 23:41 /swap 挂载swap分区： [root@PC ~]# mkswap /swap Setting up swapspace version 1, size = 1024 MiB (1073737728 bytes) no label, UUID=c3d2c1dc-adb9-468e-8ab4-fbdc8fead879 打开swap分区： [root@PC ~]# swapon /swap 设置开机启动： [root@PC ~]# vi /etc/fstab 在文件末尾添加如下代码并保存： /swap none swap sw 0 0 再次查看系统内存使用情况： [root@PC ~]# free -m total used free shared buff/cache available Mem: 493 214 6 6 272 259 Swap: 1023 0 1023 swap分区文件大小的修改就完成了！","link":"/2020/03/18/%E5%9C%A8Linux%E4%B8%8B%E4%BF%AE%E6%94%B9Swap%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F/"},{"title":"Typecho博客","text":"前一阵子腾讯云的数字域名有优惠，于是剁手了一个数字域名。有了域名，突然想起来还有个虚拟主机闲置着，就想随手搭建一个博客（虽然我很少写东西hhh）。博客系统最终选择的是typecho，非常轻量，简洁，一顿操作猛如虎之后，博客也顺利搭建成功。以后我会在这里更新我的文章！","link":"/2020/03/17/Typecho%E5%8D%9A%E5%AE%A2/"},{"title":"给博客加上HTTPS","text":"前言Trojan可以让自己的博客加上Https访问。 下面这张图是部分文档的介绍：它可以仅仅是一个Https的服务器，监听系统的443端口，展开Tls握手，握手成功后，如果判断是其他流量，就会打开一个隧道，将流量转发到预设好的端口，这个端口就可以处理解密后的Http流量了。下面这张图可以更好地理解这样的工作模式：当客户端访问服务器时，服务端监听443端口，如果是客户端的合法流量，那么就由Trojan处理，再访问互联网，如果不是客户端的合法流量（比如浏览器访问服务器上的Https网站），那么在进行Https流量解密之后，服务端会将Http流量转发给nginx监听的80端口，交给Nginx处理，从而使用户可以正常访问服务器上的网站。 开始操作 首先确认自己的博客能够正常地通过Http访问 申请一张Https证书，并准备好证书和密钥文件,放在你的服务器的合适位置 安装Trojan，使用Github上的安装脚本通过curl命令安装： sudo bash -c “$(curl -fsSL https://raw.githubusercontent.com/trojan-gfw/trojan-quickstart/master/trojan-quickstart.sh)&quot; 或者通过wget命令安装： sudo bash -c &quot;$(wget -O- https://raw.githubusercontent.com/trojan-gfw/trojan-quickstart/master/trojan-quickstart.sh)&quot; 修改/usr/local/etc/trojan/config.json文件 设置trojan自动启动 systemctl start trojansystemctl enable trojan 编辑Typecho站点根目录下的文件config.inc.php加入下面一行配置： /** 开启HTTPS */ define('__TYPECHO_SECURE__',true); 注意：Chrome内核的浏览器有不安全提示的时候，需要找到主题目录下面的comments.php文件再将其中的$this&gt;commentUrl()，替换为： echo str_replace(&quot;http&quot;,&quot;https&quot;,$this-&gt;commentUrl()); 最后保存。","link":"/2020/03/31/%E7%BB%99%E5%8D%9A%E5%AE%A2%E5%8A%A0%E4%B8%8AHTTPS/"},{"title":"爬虫获取京东手机销售数据","text":"通过爬虫获取京东手机销售数据目前消费市场上的手机品牌多种多样，比较常见的有：苹果，三星，华为，小米，VIVO，OPPO，一加，魅族，努比亚，联想，锤子，诺基亚，索尼，中兴，华硕，黑鲨等，计划通过简单的Python代码爬取京东以“手机”为关键词的前100页商品（搜索上限就是100页）的商品信息，价格，商家，评价数（销量的另一种体现），和链接，并对数据进行清洗和处理，最后存储到MySQL数据库中。 Q:为什么调查的电商平台选择京东？A: 一开始想选择淘宝作为调查的平台，因为淘宝的商品种类更加丰富。但是淘宝有非常完备的反爬虫机制，简单的爬虫程序很难爬取到有效的数据，而京东对于爬虫没有很严格的限制，选择京东作为调查的电商平台可以减少代码的工作量。 淘宝的刷单现象较为严重，而京东的刷单现象则不是很普遍，选择京东作为调查的电商平台可以让获取的数据更加真实。 京东的售后体系，特别是京东自营的商店，在售后方面更加有保障，而智能手机属于贵重物品，相比于淘宝，更多人倾向在京东购买手机。 数据采集首先对京东的网页链接进行分析：经过简化后，这个链接也可以正常使用，然后对链接的关键字进行分析：Keyword参数：就是搜索的关键字，这里我们要搜索的关键字就是“手机”。Enc参数：就是encoding编码的关键字，是网页默认的utf-8，不加这个参数会出现乱码。Page参数：是当前搜索页面的页数的关键字，第二页显示page=3，规律就是每增加一页，page会增加2，所以页数=page*2-1。接下来定位网页html代码中商品元素的位置，通过浏览器的开发者工具可以直接定位到商品元素的位置：爬取的信息选择存储到本地的MySQL数据库中，新建数据库包含的列有：爬取商品的序号，手机的名称，手机的价格，手机的评价数（销量）,销售手机的商家，手机的链接。其中，序号为表的主键，价格和评价数选择使用INT类型存储，其他均采用VARCHAR类型存储。数据库的结构如下图：接下来就可以编写代码，主要通过python下的selenium库来对网页进行爬取，其中代码如下： from selenium import webdriver from selenium.common.exceptions import TimeoutException from selenium.webdriver.common.by import By from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.support.wait import WebDriverWait from multiprocessing.pool import Pool from urllib.parse import quote from pyquery import PyQuery as pq import time import pymysql import cryptography a=1 options = webdriver.ChromeOptions() options.add_argument('--headless') browser = webdriver.Chrome(chrome_options=options) wait = WebDriverWait(browser,10) KEYWORD = '手机'#搜索的关键字是手机 headers = { 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36', 'x-requested-with':'XMLHttpRequest'#用于翻页 } def index_page(page): print('正在爬取第', page, '页') try: url = 'https://search.jd.com/Search?keyword={}&amp;enc=utf-8&amp;page={}'.format(quote(KEYWORD),page*2-1) print(url) browser.get(url) time.sleep(0.5) return browser.page_source # browser.close() except TimeoutException: index_page(page) def connect_mysql(sql):#写入到MySQL数据库 coon = pymysql.connect(host='localhost',port=3306,user='root',passwd ='123456',db ='phone',charset ='utf8') cur = coon.cursor() cur.execute(sql) if sql.strip()[:6].upper() == 'SELECT': res = cur.fetchall() else: coon.commit() res = 'ok' cur.close() coon.close() return res def get_products(html): global a # html = browser.page_source doc = pq(html) items = doc('#J_goodsList &gt; ul &gt; li').items()#全部信息都在ul下的li中 for item in items: product = { '价格': item.find('div &gt; div.p-price &gt; strong &gt; i').text(),#J_goodsList &gt; ul &gt; li:nth-child(1) &gt; div &gt; div.p-price &gt; strong &gt; i '评价数': item.find('div &gt; div.p-commit &gt; strong &gt; a').text(),#J_goodsList &gt; ul &gt; li:nth-child(1) &gt; div &gt; div.p-commit &gt; strong &gt; a '商家': item.find('div.p-shop &gt; span &gt; a').text(),#J_goodsList &gt; ul &gt; li:nth-child(1) &gt; div &gt; div.p-shop &gt; span &gt; a '手机名称': item.find('div &gt; div.p-name.p-name-type-2 &gt; a &gt; em').text(),#J_goodsList &gt; ul &gt; li:nth-child(1) &gt; div &gt; div.p-name.p-name-type-2 &gt; a &gt; em '链接': item.find('div &gt; div.p-img &gt; a').attr('href'),#J_goodsList &gt; ul &gt; li:nth-child(1) &gt; div &gt; div.p-img &gt; a } price=product['价格'] phone=product['手机名称'] mark=product['评价数'] seller=product['商家'] link=product['链接'] if price.isdigit(): continue#如果价格不是数字，跳过本次循环 mark=mark.replace('+','')#单位标准化 if '万' in mark: mark=mark.replace('万','')#单位标准化 mark=float(mark) mark=mark*10000 else: mark=float(mark) print(product) sql=&quot;INSERT INTO `phone`.`phone_list` (`序号`,`手机名称`, `价格`, `评价数(销量)`, `商家`, `链接`) VALUES ('&quot;+str(a)+&quot;', '&quot;+phone+&quot;', '&quot;+price+&quot;','&quot;+str(mark)+&quot;','&quot;+seller+&quot;','&quot;+link+&quot;');&quot; connect_mysql(sql) a+=1 if __name__ == '__main__': for i in range(1,101):#循环100次 get_products(index_page(i)) 最后一共爬取到了3000个商品，存储到数据库后的效果如下：数据采集的任务完成。 Ps：本工作完成时间为：2019-12-11，相应的数据可能与现在不同，请注意时效性。","link":"/2020/06/05/%E7%88%AC%E8%99%AB%E8%8E%B7%E5%8F%96%E4%BA%AC%E4%B8%9C%E6%89%8B%E6%9C%BA%E9%94%80%E5%94%AE%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"}]}